Given a matchup matrix, which is essentially just a payoff matrix, doing a principal component analysis should not only serve the purposes of feature extraction and dimensionality reduction, but also give us a more visual and efficient approach to finding characters to play in order to cover a wide range of matchups. The matrix has n-samples, being the characters that I play, and m-variables, being the characters that I'm concerned with playing against. The beauty of principal component analysis is that we need not even know or interpret the principal components in order to achieve our goal. Rather, all we need is a simple understanding of how it works.

With the way PCA works, and the nature of the data, the loadings and the scores establish a relationship between the samples, variables, and principal components. Since the samples, and variables are of the same medium(characters), then I will just simply denote them as "sample" vs. "variable." The loadings are give us how much each variable contributes to the variance of the data, in how it much contributes to the principal component. Some variables correlate positively, some negatively, and some not at all. In this way, in order to get the principal component scores, we relate this relationship betwen variables and principal components to the relationship between samples and variables. If the matchup matrix defines an entry, from the perspective of a row, as "how well the sample character does against the variable character," then we see that the PCA scores represent how well a sample character does against a principal component-defined type of variable character. This is just another way of expressing that the product of the transpose of the loadings matrix, and the data matrix yields the scores matrix.

Mathematically, if a sample character scores strongly positive in a principal component, then we can infer that the character does well against the variable characters that correlate strongly and positively to the principal component. Likewise, the same can be said in terms of a character whcih scores negatively. This is because the variable characters are seen as weights from the principal component they are related to, in the linear combinations that make up the scores. In the point of view of the biplots(and the triplot), as we graph both the scores as points, and the loadings as vectors, the scores which align in the direction of the loading vectors, with a greater magnitude in that direction, are the sample characters that do great against the variable characters(loading vectors). In essence, we've found a way to find which characters tend to naturally counter a wide range of other characters, and can represent this via correlational plots of the first 3 PCAs.

As an example, in principle component 1, we see that Ivysaur has the highest PCA score, and in the loadings, the variable characters that positively correlate with PC1 are Peach and Inkling. Hence, Ivysaur's pca score is a result of it doing well against both Inkling and Peach, which also passes a human judgement test, since those characters are pretty exploitable by Ivysaur. In this way, we can just use a distance function to measure the distance between the scores and the respective loadings as a metric of similarity, and ultimately, matchup viability, and if we construct a matrix out of these distances, we end up back with the matchup matrix.
